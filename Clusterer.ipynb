{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d8651b-5274-4fc6-ab22-176f456fa406",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evolution including Virus Family hybrid Clustering based on artificially mutated K-mers\n",
    "\n",
    "## Milestones\n",
    "\n",
    "- [x] HDBSCAN github errors\n",
    "    - need to find version without problems\n",
    "    - if now finding one revert back to MA version\n",
    "    - revert back to Masterthesis and update jupyter lab, git and ressource\n",
    "- [x] better inclusion of R, N, ... in the kmer\n",
    "    - implemented, maybe need adjustment by value\n",
    "    - if frature of missing higher than threshold garbage\n",
    "    - if not fill missing by possible constellations\n",
    "- [x] evolution on reading frame\n",
    "    - difficult with amino conservation ORF tracker necessary\n",
    "        - e.g. BLOSUM etc.\n",
    "    - nucleotide exchange values used now, instead of amino exchange\n",
    "        - usage of Kimura's two-parameter model\n",
    "        - alpha and beta of user choice\n",
    "- [ ] stable parameters \n",
    "    - best would be algorithmic solution here\n",
    "    - number of clusters\n",
    "        - neighbors -> distance matrix -> kneedle algorithm -> epsilon\n",
    "    - sample number\n",
    "        - cluster number extraction algorithms -> sample \n",
    "    - alpha value (A -> G, C -> T)\n",
    "    - beta value (...)\n",
    "\n",
    "## Implementation Blueprint\n",
    "\n",
    "![Class2](Clusterer.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e43f69-8d14-4baf-bfa0-77576b87acfa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "renewable-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import math\n",
    "import re\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "import multiprocessing as mp\n",
    "import hdbscan\n",
    "import progressbar\n",
    "import scipy.spatial.distance as ssd\n",
    "from scipy.cluster import hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d490fa0-5589-4c97-b50a-234455ed43f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classes and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "noticed-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectors(object):\n",
    "    \n",
    "    def __init__(self, k = 7, identifier = 0, split = None, quality = {'':0}, variable = 0.9, state = 1.0, alpha = 0.0, beta = 0.0, init = 0.0, procs = 4):\n",
    "    \n",
    "        self.k = k\n",
    "        self.quality = quality\n",
    "        self.identifier = identifier\n",
    "        self.split = split\n",
    "        self.variable = variable\n",
    "        self.nucleotides = ['A', 'C', 'G', 'T']\n",
    "        self.substit = dict.fromkeys(map(ord, self.nucleotides), None)\n",
    "        self.exist = dict.fromkeys(map(''.join, it.product(self.nucleotides, repeat = self.k)), init)        \n",
    "        self.col = len(self.exist.keys())\n",
    "        self.state = state\n",
    "        self.procs = procs\n",
    "        self.exchange = {\n",
    "            'A':['A'],\n",
    "            'C':['C'],\n",
    "            'G':['G'],\n",
    "            'T':['T'],\n",
    "            'R':['A', 'G'],\n",
    "            'Y':['C', 'T'],\n",
    "            'W':['A', 'T'],\n",
    "            'S':['C', 'G'],\n",
    "            'M':['A', 'C'],\n",
    "            'K':['G', 'T'],\n",
    "            'B':['G', 'C', 'T'],\n",
    "            'H':['A', 'C', 'T'],\n",
    "            'D':['A', 'G', 'T'],\n",
    "            'V':['A', 'C', 'G'],\n",
    "            'N':['A', 'C', 'G', 'T'],\n",
    "        } \n",
    "        self.kimura = {\n",
    "            'A':{'A':state, 'C':beta, 'G':alpha, 'T':beta,},\n",
    "            'C':{'A':beta, 'C':state, 'G':beta, 'T':alpha,},\n",
    "            'G':{'A':alpha, 'C':beta, 'G':state, 'T':beta,},\n",
    "            'T':{'A':beta, 'C':alpha, 'G':beta, 'T':state,},\n",
    "        }\n",
    "    \n",
    "    def countRows(self, infile):\n",
    "        \n",
    "        sequences = {}\n",
    "        index = []\n",
    "        row = 0\n",
    "        for entry in SeqIO.parse(infile,'fasta'):\n",
    "            \n",
    "            #name = entry.description\n",
    "            #header = entry.name.split(self.split)\n",
    "            header = entry.description.split(self.split)\n",
    "            sequence = str(entry.seq)\n",
    "            missing = len(sequence.translate(self.substit))\n",
    "            fracture = float(len(sequence)/missing) if missing else 0 \n",
    "            #accession = header[0]#.split(':')[1]\n",
    "            accession = header[self.identifier] if type(self.identifier) == int else re.search('.*' + self.identifier + '([^|]+).*', entry.description)[1]\n",
    "            \n",
    "            try:\n",
    "                if all([re.match(i, header[self.quality[i]], re.IGNORECASE) for i in self.quality]) == True and fracture <= self.variable:\n",
    "                    row += 1\n",
    "                    sequences[accession] = sequence\n",
    "                    index.append(accession)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return(row, index, sequences)\n",
    "    \n",
    "    def calculateKmer(self, sequence):\n",
    "        \n",
    "        temporary = self.exist.copy()\n",
    "        for i in range(len(sequence) - self.k + 1):\n",
    "            kmer = sequence[i:i+self.k]\n",
    "            main = map(''.join, it.product(*[self.exchange.get(j) for j in kmer]))\n",
    "\n",
    "            for sub in main:\n",
    "                for l, nuc in enumerate(sub):\n",
    "                    for mut in self.nucleotides:\n",
    "                        mutation = sub[:l] + mut + sub[l+1:]\n",
    "                        temporary[mutation] += self.kimura[nuc][mut]\n",
    "\n",
    "        vector = np.fromiter(temporary.values(), dtype = 'float32', count = self.col)/sum(temporary.values())\n",
    "        temporary.clear()\n",
    "        return(vector)\n",
    "        \n",
    "    def calculateFrequence(self, infile):\n",
    "        \n",
    "        row, index, sequences = self.countRows(infile)\n",
    "        matrix = np.empty((row, self.col, ),dtype = 'float32')\n",
    "        \n",
    "        widgets = [' [', progressbar.Timer(format = 'elapsed time: %(elapsed)s'), '] ', progressbar.Bar('#'),' (', progressbar.ETA(), ') ', ]\n",
    "        bar = progressbar.ProgressBar(max_value = len(index), widgets = widgets).start()\n",
    "\n",
    "        with mp.Pool(self.procs) as pool:\n",
    "            result = pool.imap(self.calculateKmer, map(lambda m: sequences[m], index))\n",
    "            for pos, vector in enumerate(result):\n",
    "                matrix[pos] = vector\n",
    "                bar.update(pos)\n",
    "            \n",
    "        bar.finish()\n",
    "\n",
    "        return(index, matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb81e31-253d-41d3-aebd-8e15971c31ec",
   "metadata": {},
   "source": [
    "- execution can still be faster ca. 15-20min for segment 4 is still slow\n",
    "    - inclusion of mutation increased the runtime by factor 5-10\n",
    "    - multiprocessing difficult to implement (dicts, fast calculation of single instances high overhang)\n",
    "- all mutations and all unkown kmers (including e.g. Ns) are counted with state or respective alpha beta\n",
    "    - maybe split value by their number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6afb4be1-5c98-4a74-baf2-991f0b2e0225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cluster(linkage, min_clust, num_clust):\n",
    "\n",
    "    x = 0.0\n",
    "    y = 1.0\n",
    "    cluster = linkage.get_clusters(cut_distance = x, min_cluster_size = min_clust)\n",
    "    n = cluster.max().item()\n",
    "    \n",
    "    while n != num_clust:\n",
    "\n",
    "        if n < num_clust and n != -1:\n",
    "            x = x - y\n",
    "            y = y * 0.1\n",
    "\n",
    "        else:\n",
    "            x = x + y\n",
    "\n",
    "        cluster = linkage.get_clusters(cut_distance = x, min_cluster_size = min_clust)\n",
    "        n = cluster.max().item()\n",
    "        \n",
    "        if x != 0.0 and n == -1:\n",
    "            print('Error.')\n",
    "            break\n",
    "        \n",
    "    return(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e03007cf-0d79-469f-995d-d65dee6b885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def workerCentroid(subl2):\n",
    "    \n",
    "    tuple_min = (0, '')\n",
    "    for name, row in subl2.iterrows():\n",
    "\n",
    "        if not tuple_min[1]:\n",
    "            dist_min = ssd.cdist([row], subl2, 'euclidean').mean()\n",
    "            tuple_min = (dist_min, name)\n",
    "        else:\n",
    "            dist_min = ssd.cdist([row], subl2, 'euclidean').mean()\n",
    "            if dist_min < tuple_min[0]:\n",
    "                tuple_min = (dist_min, name)\n",
    "                \n",
    "    return(tuple_min[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6998eb7d-d39d-4087-80c2-f6df034dbd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Centroid(framecluster, framel2):\n",
    "    \n",
    "    groups = framecluster.query('cluster != -1').groupby('cluster').groups\n",
    "    \n",
    "    with mp.Pool(procs) as pool:\n",
    "        result = pool.imap(workerCentroid, map(lambda match: framel2.loc[match], groups.values()))\n",
    "        centroid = pd.DataFrame([(index, i) for i, index in enumerate(result)], columns = ['accession', 'cluster']).set_index('accession')\n",
    "        \n",
    "    return(centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5056fe9c-1a70-42df-a2bd-6f023476dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linkage(framecentroid, framel2):\n",
    "    \n",
    "    subl2 = framel2.loc[framecentroid.index.tolist()]\n",
    "    distance = ssd.cdist(subl2, subl2, 'euclidean')\n",
    "    linkage = hierarchy.linkage(distance, method = 'single', metric = 'euclidean')\n",
    "    \n",
    "    return(linkage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1629877e-78f0-41a7-b1d4-72240525bbef",
   "metadata": {},
   "source": [
    "- needs some kind of error correction e.g. when only 4 sequences 60 clusters are impossible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784f0866-79db-4c34-ad4f-6b0f2c927248",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a2b792e-80db-4f31-8085-18ea98ac84ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 7\n",
    "split = '|'\n",
    "quality = {'(?!^UNKNOWN_.*)':1}\n",
    "#quality = {'':0}\n",
    "identifier = 0\n",
    "variable = 0.9\n",
    "min_clust = 2\n",
    "sample = 1\n",
    "num_clust =  60\n",
    "n_components = 50\n",
    "procs = 16\n",
    "state = 1.0\n",
    "alpha = 0.01\n",
    "beta = 0.005\n",
    "init = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "harmful-tuesday",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [elapsed time: 0:02:30] |##################################| (Time:  0:02:30) \n"
     ]
    }
   ],
   "source": [
    "vectors = Vectors(k = k, identifier = identifier, split = split, quality = quality, variable = variable, state = state, alpha = alpha, beta = beta, init = init, procs = procs)\n",
    "index, matrixl1 = vectors.calculateFrequence('degue.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "illegal-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 50)\n",
    "matrixpca = pca.fit_transform(matrixl1)\n",
    "variance = pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "collected-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrixl2 = normalize(matrixpca, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6dc9880-ec6c-4b7e-94ee-efb07a37f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbinit = hdbscan.HDBSCAN(min_samples = sample, min_cluster_size = min_clust, gen_min_span_tree = True, metric = 'euclidean').fit(matrixl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bff8f94-97bd-46e4-a70b-d7bfa96527cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = Cluster(linkage, min_clust, num_clust) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3bf2af4e-6c6a-4962-aa83-5d468e310b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "framecluster = pd.DataFrame(cluster, columns = ['cluster'], index = index)\n",
    "framel2 = pd.DataFrame(normalize(matrixl1, norm='l2'), index = index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "047064fd-2485-4997-a509-bf8bcf86451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "framecentroid = Centroid(framecl, framel2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0d3db667-ff37-4e59-9b8c-218e45269185",
   "metadata": {},
   "outputs": [],
   "source": [
    "unclustered = len(framecl.query('cluster == -1'))\n",
    "clustered = len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "509ec6f8-e6bc-49f6-bc1c-8993af8d71a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-152-af985b35d8d4>:5: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  linkage = hierarchy.linkage(distance, method = 'single', metric = 'euclidean')\n"
     ]
    }
   ],
   "source": [
    "#linkage = hdbinit.single_linkage_tree_\n",
    "#framelinkage = linkage.to_pandas().drop(['parent'], axis=1)#.set_index('parent', inplace = False)\n",
    "linkage = Linkage(framecentroid, framel2)\n",
    "framelinkage = pd.DataFrame(linkage, columns = ['left_child', 'right_child', 'distance', 'size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0b0aaefe-1a24-4829-84dc-b9154de87cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "framecluster.to_csv('cluster.csv', index=True, header=True, sep=',', mode='w')\n",
    "framelinkage.to_csv('linkage.csv', index=False, header=True, sep=',', mode='w')\n",
    "framecentroid.to_csv('centroid.csv', index=True, header=True, sep=',', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a46c7-d484-41ce-9e33-f53914fa6a67",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "![Result](Result.svg)\n",
    "\n",
    "- NO CHANGE of H7 and H15 and H4 and H14!\n",
    "    - mixing of subtypes correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625db32a-1deb-4ed2-a5f8-0f1bfdcfef57",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Garbage Place"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5697df5a-f35c-43de-8ddc-9a73fca5c02a",
   "metadata": {
    "tags": []
   },
   "source": [
    "from itertools import repeat\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "def f(d: dict) -> None:\n",
    "    pid = os.getpid()\n",
    "    d[pid] = \"Hi, I was written by process %d\" % pid\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with mp.Manager() as manager:\n",
    "        d = manager.dict()\n",
    "        with manager.Pool() as pool:\n",
    "            pool.map(f, repeat(d, 10))\n",
    "        # `d` is a DictProxy object that can be converted to dict\n",
    "        pprint.pprint(dict(d))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82913ae1-85fa-4ac1-bd51-6bd30973bd94",
   "metadata": {
    "tags": []
   },
   "source": [
    "def calculateFragment(self, accession):\n",
    "        \n",
    "        sequence = self.sequences[accession]\n",
    "        for i in range(len(sequence) - self.k + 1):\n",
    "\n",
    "            kmer = sequence[i:i+self.k]\n",
    "            main = map(''.join, it.product(*[self.nucex.get(j) for j in kmer]))\n",
    "\n",
    "            for sub in main:\n",
    "                self.exist[sub] += self.state\n",
    "                for l, nuc in enumerate(sub):\n",
    "                    for mut in self.nucmut[nuc]:\n",
    "                        mutation = sub[:l] + mut + sub[l+1:]\n",
    "                        self.exist[mutation] += self.nucval[(nuc,mut)]\n",
    "\n",
    "        vector = np.fromiter(self.exist.values(), dtype = 'float32', count = self.col)/sum(self.exist.values())\n",
    "        return(vector)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1bf4682-7af0-4250-84e3-ca9f6382b26e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#from kneed import DataGenerator, KneeLocator\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#from scipy.signal import savgol_filter\n",
    "#from sklearn.neighbors import NearestNeighbors\n",
    "#import networkx as nx"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a0cbb5c-2265-4d07-9b8d-b59486d7c4b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "                #    size = np.prod([len(self.nucex.get(k)) for k in kmer])\n",
    "                # for sub in main:\n",
    "                #    self.exist[sub] += float(1/size)\n",
    "                #    self.exist[sub] += float((1-self.mutfac)/size)     \n",
    "                #    for l, nuc in enumerate(sub):\n",
    "                #        for mutation in map(''.join, it.product(*[[sub[:l]], self.nucmut.get(nuc), [sub[l+1:]]])):\n",
    "                #            self.exist[mutation] += float(self.mutfac/(size*12))\n",
    "                #        for mutation in map(''.join, it.product(*[[sub[:l]], self.nucmut.get(nuc), [sub[l+1:]]])):\n",
    "                #            self.exist[mutation] += self.nucval[(nuc,mutation[l])]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7aeaf683-1d03-4f3c-9c4d-be4ed89d1f63",
   "metadata": {
    "tags": []
   },
   "source": [
    "def get_elbow(dataframe, index, min_clust, sample, kneedle):\n",
    "    \n",
    "    #with some help from https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/\n",
    "    \n",
    "    clusterer_best = hdbscan.HDBSCAN(\n",
    "        min_samples = sample,\n",
    "        min_cluster_size = min_clust,\n",
    "        gen_min_span_tree = True,\n",
    "        metric = 'euclidean',\n",
    "    ).fit(dataframe)\n",
    "    \n",
    "    label = clusterer_best.labels_\n",
    "    label_list = label.tolist()\n",
    "    \n",
    "    if -1 in label_list:\n",
    "        label_list.remove(-1)\n",
    "    \n",
    "    n_cluster = len(set(label_list))\n",
    "    \n",
    "    linkage = clusterer_best.single_linkage_tree_.to_pandas()\n",
    "    numpy_linkage = linkage.drop(columns=['parent']).to_numpy()\n",
    "    \n",
    "    dist = numpy_linkage[:, 2]\n",
    "    dist_rev = dist[::-1]\n",
    "    \n",
    "    if kneedle == -1:\n",
    "        dist_area = dist_rev[0:n_cluster]\n",
    "    else:\n",
    "        dist_area = dist_rev[0:kneedle]\n",
    "    \n",
    "    idxs = np.arange(1, len(dist_area) + 1)\n",
    "\n",
    "    kn = KneeLocator(idxs, dist_area,\n",
    "        curve='convex',\n",
    "        direction='decreasing',\n",
    "        interp_method='polynomial',\n",
    "        online=True,\n",
    "        S = 1.0,\n",
    "    )\n",
    "    \n",
    "    n_cluster_raw = kn.knee\n",
    "    n_cluster_norm = kn.norm_knee\n",
    "    \n",
    "    #linkage.set_index('parent', inplace = True)\n",
    "    elbow = pd.DataFrame({'n_cluster': idxs, \n",
    "                          'distance': dist_area, \n",
    "                          'x_normalized':kn.x_normalized, \n",
    "                          'y_normalized':kn.y_normalized, \n",
    "                          'x_difference':kn.x_difference, \n",
    "                          'y_difference':kn.y_difference}).set_index('n_cluster')\n",
    "    epsilon_best = elbow.loc[n_cluster_raw][['distance']].item()\n",
    "    linkage.set_index('parent', inplace = True)\n",
    "    \n",
    "    return(linkage, elbow, epsilon_best, n_cluster_raw, n_cluster_norm)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76cc2df0-3770-4778-a576-eca2a198bc6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "def get_cluster(epsilon_best, dataframe, accession, min_clust, sample):\n",
    "    \n",
    "    clusterer_best = hdbscan.HDBSCAN(\n",
    "        min_samples = sample,\n",
    "        min_cluster_size = min_clust,\n",
    "        cluster_selection_epsilon = epsilon_best,\n",
    "        gen_min_span_tree=True,\n",
    "        metric = 'euclidean',\n",
    "    ).fit(dataframe)\n",
    "\n",
    "    label = clusterer_best.labels_\n",
    "    label_list = label.tolist()\n",
    "    \n",
    "    unclustered = label_list.count(-1)\n",
    "    label_set = set(label_list)\n",
    "    \n",
    "    if -1 in label_set:\n",
    "        label_set.remove(-1)\n",
    "    \n",
    "    n_cluster = len(label_set)\n",
    "    \n",
    "    cluster = pd.DataFrame(zip(index, label), columns = ['accession', 'cluster']).set_index('accession')\n",
    "    \n",
    "    return(cluster, n_cluster, unclustered)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1a6b0fe5-6dd6-4bf7-a705-83b9711748c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "epsilon_best"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59f39703-9629-44f0-94fd-512d26c218bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "cluster, n_cluster, unclustered = get_cluster(epsilon_best, matrixl2, index, min_clust, 2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18c1a533-be86-44af-a53a-b3280442e07e",
   "metadata": {
    "tags": []
   },
   "source": [
    "print(n_cluster, unclustered)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ec2359f-8cb5-4ff7-a1ee-ee0b67cf3b7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "cluster.to_csv('cluster.csv', index=True, header=True, sep=',', mode='w')\n",
    "linkage.to_csv('linkage.csv', index=True, header=True, sep=',', mode='w')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8259a63b-7307-4536-81e6-179712b61865",
   "metadata": {
    "tags": []
   },
   "source": [
    "pd.DataFrame(matrixl2, index = index).to_csv('matrixl2.csv', index=True, header=True, sep=',', mode='w')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a6c4649-f324-4a88-80f8-f0c40020ad49",
   "metadata": {
    "tags": []
   },
   "source": [
    "#linkage, elbow, epsilon_best, n_cluster_raw, n_cluster_norm = get_elbow(matrixl2, index, min_clust, sample, kneedle)\n",
    "epsilon_best = get_elbow2(matrixl2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58b9d118-199c-4ab9-8c89-a2fd353e6fb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "def get_elbow2(matrixl2):\n",
    "\n",
    "    nearest_neighbors = NearestNeighbors(n_neighbors=11)\n",
    "    neighbors = nearest_neighbors.fit(matrixl2)\n",
    "    distances, indices = neighbors.kneighbors(matrixl2)\n",
    "\n",
    "    distances = np.sort(distances[:,10], axis=0)\n",
    "    i = np.arange(len(distances))\n",
    "    knee = KneeLocator(i, distances, S=10, curve='convex', direction='increasing', interp_method='polynomial')\n",
    "    knee.plot_knee()\n",
    "    \n",
    "    return(distances[knee.knee].item())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6280c61b-dd2b-4443-8c25-bf561490a3cb",
   "metadata": {},
   "source": [
    "kimura = {\n",
    "    'A':{'A':state, 'C':beta, 'G':alpha, 'T':beta,},\n",
    "    'C':{'A':beta, 'C':state, 'G':beta, 'T':alpha,},\n",
    "    'G':{'A':alpha, 'C':beta, 'G':state, 'T':beta,},\n",
    "    'T':{'A':beta, 'C':alpha, 'G':beta, 'T':state,},\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a86d5eed-e977-4790-b677-5740e938fde1",
   "metadata": {},
   "source": [
    "next(map(lambda x: kimura[x], ['A', 'C', 'G', 'T']))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b57adc96-b0a4-4951-8c38-ea22a8686ca5",
   "metadata": {},
   "source": [
    "map(lambda x: x*2, [1, 2, 3])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a746ea59-3253-402a-b6c5-b59f958ae69e",
   "metadata": {},
   "source": [
    "next(map(lambda x: x*2, [1, 2, 3]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "052d6015-4423-449e-8b8e-6dc1626a1c46",
   "metadata": {},
   "source": [
    "for i in range(len(sequence) - self.k + 1):\n",
    "kmer = sequence[i:i+self.k]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5aea751e-4f62-4b3f-a275-2c2d456ff022",
   "metadata": {},
   "source": [
    "def split_seq(seq,size):\n",
    "    \"\"\" Split up seq in pieces of size \"\"\"\n",
    "    return [seq[i:i+size] for i in range(0, len(seq), size)]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
